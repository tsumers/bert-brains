{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT attention heads\n",
    "\n",
    "Going deeper on the BERT representations.\n",
    "\n",
    "See:  \n",
    "https://huggingface.co/transformers/bertology.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Clark et al's analysis of BERT's attention heads:  \n",
    "https://www-nlp.stanford.edu/pubs/clark2019what.pdf\n",
    "\n",
    "![title](data/coref_head.png)\n",
    "\n",
    "Let's see if we can grab head 5-4 and confirm this pattern of attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "\n",
    "# All the classes for an architecture can be initiated from pretrained weights for this architecture\n",
    "# Note that additional weights added for fine-tuning are only initialized\n",
    "# and need to be trained on the down-stream task\n",
    "pretrained_weights = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "model = BertModel.from_pretrained(pretrained_weights)\n",
    "\n",
    "# Models can return full list of hidden-states & attentions weights at each layer\n",
    "model = BertModel.from_pretrained(pretrained_weights,\n",
    "                                  output_hidden_states=True,\n",
    "                                  output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sentence from the paper:\n",
    "input_ids = torch.tensor([tokenizer.encode(\"joining peace talks between Israel and the Palestinians. The negotiations are\")])\n",
    "all_hidden_states, all_attentions = model(input_ids)[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Attention Weights\n",
    "\n",
    "From: https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "\n",
    "**attentions**: Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads, sequence_length, sequence_length).\n",
    "\n",
    "Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 12])\n",
      "\"Negotiations\" attention weights: \n",
      "\ttensor([0.0180, 0.0123, 0.3793, 0.0214, 0.0043, 0.0070, 0.0026, 0.0116, 0.0046,\n",
      "        0.0022, 0.5357, 0.0011], grad_fn=<SelectBackward>)\n",
      "\n",
      "\"Talks\" attention weights: \n",
      "\ttensor([1.5396e-02, 2.9177e-03, 5.4983e-01, 2.8408e-03, 3.1879e-04, 5.3173e-04,\n",
      "        3.1515e-04, 8.3999e-04, 4.0847e-03, 3.0516e-03, 4.1416e-01, 5.7109e-03],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "coref_head = all_attentions[4][0][3]\n",
    "\n",
    "print(coref_head.shape)\n",
    "\n",
    "# Look at the references for the two co-referent words:\n",
    "print(\"\\\"Negotiations\\\" attention weights: \\n\\t{}\".format(coref_head[-2]))\n",
    "\n",
    "print(\"\\n\\\"Talks\\\" attention weights: \\n\\t{}\".format(coref_head[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
